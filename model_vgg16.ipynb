{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_vgg16.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yO1MC8kA7xB"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from tensorflow.keras.utils import to_categorical #using ternsorflow to inport keras.utils (deu pau pra importar só do Keras (ainda não sei pq)\n",
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LsJSgd0BBmd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',  force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nam6n12XiW5"
      },
      "source": [
        "TRAIN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAyzgA6UYbI2"
      },
      "source": [
        "from tensorflow.keras.applications import VGG16 # same keras bug here (não consegui usar \"from keras.applications import VGG16\" acho que isso obriga a usar o tensorflow no background)\n",
        "\n",
        "vgg_conv = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(256, 256, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXmJc6IJYK30"
      },
      "source": [
        "#Base directory\n",
        "\n",
        "train_dir = '/content/drive/My Drive/cancer_detection/paper/model_2/patch_train'\n",
        "validation_dir = '/content/drive/My Drive/cancer_detection/paper/model_2/patch_val'\n",
        "test_dir = '/content/drive/My Drive/cancer_detection/paper/model_2/patch_test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vxvwAg4Yt8_"
      },
      "source": [
        "vgg_conv.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhKpe0dpY1mY"
      },
      "source": [
        "# this is the augmentation configuration we will use for training\n",
        "data_augmentation = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True)\n",
        "\n",
        "batch_size = 20\n",
        "num_train = 206\n",
        "\n",
        "train_patches = np.zeros(shape=(num_train, 8, 8, 512)) # I have to study how does dimensions works ---> DISCUSSING POINT <---\n",
        "train_labels = np.zeros(shape=(num_train,2))\n",
        "\n",
        "train_generator = data_augmentation.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "i = 0\n",
        "for inputs_batch, labels_batch in train_generator:\n",
        "    patches_batch = vgg_conv.predict(inputs_batch)\n",
        "    train_patches[i * batch_size : (i + 1) * batch_size] = patches_batch\n",
        "    train_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "    i += 1\n",
        "    if i * batch_size >= num_train: # to stop and to do not generate a loop\n",
        "        break\n",
        "        \n",
        "train_patches = np.reshape(train_patches, (num_train, 8 * 8 * 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_k4Gu2iesT3"
      },
      "source": [
        "num_val = 55\n",
        "\n",
        "data_augmentation = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_patches = np.zeros(shape=(num_val, 8, 8, 512))\n",
        "validation_labels = np.zeros(shape=(num_val,2))\n",
        "\n",
        "validation_generator = data_augmentation.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "i = 0\n",
        "for inputs_batch, labels_batch in validation_generator:\n",
        "    patches_batch = vgg_conv.predict(inputs_batch)\n",
        "    validation_patches[i * batch_size : (i + 1) * batch_size] = patches_batch\n",
        "    validation_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "    i += 1\n",
        "    if i * batch_size >= num_val:\n",
        "        break\n",
        "\n",
        "validation_patches = np.reshape(validation_patches, (num_val, 8 * 8 * 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV9rMUP1fIXy"
      },
      "source": [
        "num_test = 57\n",
        "\n",
        "test_patches = np.zeros(shape=(num_test, 8, 8, 512))\n",
        "test_labels = np.zeros(shape=(num_test,2))\n",
        "\n",
        "data_augmentation = data_augmentation = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = data_augmentation.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "i = 0\n",
        "for inputs_batch, labels_batch in test_generator:\n",
        "    patches_batch = vgg_conv.predict(inputs_batch)\n",
        "    test_patches[i * batch_size : (i + 1) * batch_size] = patches_batch\n",
        "    test_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "    i += 1\n",
        "    if i * batch_size >= num_test:\n",
        "        break\n",
        "\n",
        "test_patches = np.reshape(test_patches, (num_test, 8 * 8 * 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0zcg0Z5fw8S"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_lsFdn0f200"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_dim=8 * 8 * 512)) # I need to study how does dimensions works. Define best architecture ---> DISCUSSING POINT <---\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=2e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_patches,\n",
        "                    train_labels,\n",
        "                    epochs=20,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(validation_patches,validation_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuzZvjd7gi0O"
      },
      "source": [
        "SAVE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOXg299TgYkD"
      },
      "source": [
        "# Save the Model\n",
        "\n",
        "model_dir = '/content/drive/My Drive/cancer_detection/paper/model_2'\n",
        "\n",
        "model_name = 'model_vgg_cancer_vs2.h5' \n",
        "#model_vgg_cancer_all_vs1.h5 -> all methods was implemented\n",
        "#model_vgg_cancer_vs2.h5 -> stain norm was not implemented\n",
        "\n",
        "model.save(os.path.join(model_dir, model_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9n5Iw4KggZX"
      },
      "source": [
        "# Recreating same model\n",
        "new_model = keras.models.load_model(os.path.join(model_dir, model_name))\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAXLtnY0gnS5"
      },
      "source": [
        "loss, acc = new_model.evaluate(validation_patches,validation_labels)\n",
        "print(\"Model validation accuracy: {:5.2f}%\".format(100*acc))\n",
        "\n",
        "loss, acc = new_model.evaluate(test_patches,test_labels)\n",
        "print(\"Model test accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YpyJSiUhcW1"
      },
      "source": [
        "# Plot the accuracy and loss curves ---> nedd to undertand better this graph\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}